{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMh7IStK48JCJXQMZCQXP2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/egipot/Python_simple_projects/blob/main/edx_IBM_BD0225EN_4d_User_Defined_Schema_for_DSL_and_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reading: User-Defined Schema (UDS) for DSL and SQL\n",
        "\n",
        "*Estimated time needed: 10 minutes*\n",
        "\n",
        "How to Define and Enforce a User-Defined Schema in PySpark?\n",
        "\n",
        "In this reading, you will learn how to define and enforce a user-defined schema in PySpark.\n",
        "\n",
        "Spark provides a structured data processing framework that can define and enforce schemas for various data sources, including CSV files. Let's look at the steps to define and use a user-defined schema for a CSV file in PySpark:\n",
        "\n",
        "Step 1:\n",
        "\n",
        "Import the required libraries."
      ],
      "metadata": {
        "id": "St6TDp6x_eih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ls6ogIi0_HdL",
        "outputId": "c0f8159e-7563-4754-bf1d-9f8cd98274af"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=7a313ff16005616367b5774555b80de700dbdc9ccca3c7ce2ff5e4df3197a94d\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MhkB8Upe-AFe"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql.types import StructType, IntegerType, FloatType, StringType, StructField"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2:\n",
        "\n",
        "Define the schema.\n",
        "\n",
        "Understanding the data before defining a schema is an important step.\n",
        "\n",
        "Let's take a look at the step-by-step approach to understanding the data and defining an appropriate schema for a given input file:\n",
        "\n",
        "Explore the data: Understand the different data types present in each column.\n",
        "\n",
        "Column data types: Determine the appropriate data types for each column based on your observed values.\n",
        "\n",
        "Define the schema: Use the 'StructType' class in Spark and create a 'StructField' for each column, mentioning the column name, data type, and other properties.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "pWTevZZQ_tPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([\n",
        "    StructField(\"Emp_Id\", StringType(), False),\n",
        "    StructField(\"Emp_Name\", StringType(), False),\n",
        "    StructField(\"Department\", StringType(), False),\n",
        "    StructField(\"Salary\", IntegerType(), False),\n",
        "    StructField(\"Phone\", IntegerType(), True),\n",
        "])"
      ],
      "metadata": {
        "id": "yB6n9o1d_uAu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'False' indicates null values are NOT allowed for the column.\n",
        "\n",
        "The schema defined above can be utilized for the below CSV file data:\n",
        "\n",
        "Filename: employee.csv"
      ],
      "metadata": {
        "id": "QhdKeVGQ_xgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pyblot added:\n",
        "#Using the Interactive sheets in Google Colab, create the employee.csv file\n",
        "\n",
        "from google.colab import sheets\n",
        "\n",
        "# Create a new interactive sheet and add data to it.\n",
        "sheet = sheets.InteractiveSheet()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "id": "8GKI7CBG_0Jg",
        "outputId": "4e423291-eab1-47dc-b963-ac8385dc8ebe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://docs.google.com/spreadsheets/d/13UwEVNmNJtuY0Jrx6OEmeGtNV-XP9Pi8iu6x2Wdoc_o#gid=0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7dd872512f20>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"600\"\n",
              "            src=\"https://docs.google.com/spreadsheets/d/13UwEVNmNJtuY0Jrx6OEmeGtNV-XP9Pi8iu6x2Wdoc_o#gid=0/edit?rm=embedded?usp=sharing?widget=true&amp;headers=false\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "File >> Download as csv, then upload the csv file into the workspace\n",
        "\n"
      ],
      "metadata": {
        "id": "OVDni-XZBTfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Read the input file with user-defined schema."
      ],
      "metadata": {
        "id": "FvOnpplnBZao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first import the SparkSession class from pyspark.sql. Then, we create a SparkSession object named spark using the builder pattern. Now you can use this spark object to read your CSV file."
      ],
      "metadata": {
        "id": "weIzLyokDWR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://spark.apache.org/docs/latest/api/python/index.html\n",
        "#https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html\n",
        "\n",
        "# Import necessary modules\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"CSVReader\").getOrCreate()\n",
        "\n",
        "#create a dataframe on top a csv file\n",
        "df = (spark.read\n",
        "  .format(\"csv\")\n",
        "  .schema(schema)\n",
        "  .option(\"header\", \"true\")\n",
        "  .load(\"employee.csv\")\n",
        ")\n",
        "# display the dataframe content\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66EjfvwSBY70",
        "outputId": "531b86e4-72a9-45bd-fa2e-587aee3b06f8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+----------------+------+-----+\n",
            "|Emp_Id|Emp_Name|      Department|Salary|Phone|\n",
            "+------+--------+----------------+------+-----+\n",
            "|  A101|    jhon|computer science|  1000| NULL|\n",
            "|  A102|   Peter|     Electronics|  2000| NULL|\n",
            "|  A103| Micheal|              IT|  2500| NULL|\n",
            "+------+--------+----------------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Use the printSchema() method in Spark to display the schema of a DataFrame and ensure that the schema is applied correctly to the data."
      ],
      "metadata": {
        "id": "34kquFDsDbCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFibgTUhDcVf",
        "outputId": "03196463-ad54-4949-ea83-0813e69e5fea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Emp_Id: string (nullable = true)\n",
            " |-- Emp_Name: string (nullable = true)\n",
            " |-- Department: string (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            " |-- Phone: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Through the preceding four steps, you've acquired the ability to establish a schema for a CSV file. Additionally, you've employed this user-defined schema (UDF) to read the CSV file, exhibit its contents, and showcase the schema itself."
      ],
      "metadata": {
        "id": "fk5JmOTuDhQ4"
      }
    }
  ]
}